{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-victoria",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adopted-annex",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:46.105219Z",
     "start_time": "2021-04-14T19:12:45.776689Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-diversity",
   "metadata": {},
   "source": [
    "# Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-alert",
   "metadata": {},
   "source": [
    "## Basic sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quarterly-parker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:46.601155Z",
     "start_time": "2021-04-14T19:12:46.595813Z"
    }
   },
   "outputs": [],
   "source": [
    "def basic_rand_sampler(seq, sample_len):\n",
    "    \"\"\"\n",
    "    Basic random text sampler.\n",
    "    If sample_len is greater than the length of the seq, the seq is returned.\n",
    "    \"\"\"\n",
    "    seq_len   = len(seq)\n",
    "    if seq_len > sample_len:\n",
    "        start_idx = random.randint(0, min(seq_len,seq_len - sample_len))\n",
    "        end_idx   = start_idx+sample_len\n",
    "        return seq[start_idx:end_idx]\n",
    "    else:\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proprietary-classroom",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:46.802630Z",
     "start_time": "2021-04-14T19:12:46.797092Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"ABC DEF GHI JKL!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "central-cocktail",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:46.921563Z",
     "start_time": "2021-04-14T19:12:46.894716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F GHI JK', 'C DEF GH', 'ABC DEF ', 'C DEF GH', 'GHI JKL!', 'F GHI JK']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[basic_rand_sampler(text, 8) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "worldwide-personal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:47.075563Z",
     "start_time": "2021-04-14T19:12:47.066472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABC DEF GHI JKL!', 'ABC DEF GHI JKL!', 'ABC DEF GHI JKL!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[basic_rand_sampler(text, 200) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-hypothetical",
   "metadata": {},
   "source": [
    "## Identity sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prescription-dodge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:47.454034Z",
     "start_time": "2021-04-14T19:12:47.448732Z"
    }
   },
   "outputs": [],
   "source": [
    "identity_sampler = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recreational-plane",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:47.756744Z",
     "start_time": "2021-04-14T19:12:47.751205Z"
    }
   },
   "outputs": [],
   "source": [
    "assert text == identity_sampler(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-match",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-toolbox",
   "metadata": {},
   "source": [
    "## Basic aminoacid tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "broad-nomination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:48.578718Z",
     "start_time": "2021-04-14T19:12:48.572763Z"
    }
   },
   "outputs": [],
   "source": [
    "def basic_aa_tokenizer(seq, context_length, return_mask=True):\n",
    "    \"\"\"\n",
    "    Maps a number between 0 and 21 to each 21 proteogenic aminoacids.\n",
    "    Unknown char input gets mapped to 22.\n",
    "    \"\"\"\n",
    "    aa = \"ACDEFGHIKLMNOPQRSTUVWY\"\n",
    "    d = {a: i for i, a in enumerate(aa)}\n",
    "    seq_len = len(seq)\n",
    "    seq_empty = torch.zeros(context_length - len(seq))\n",
    "    seq_tok   = torch.tensor([d[a] if a in aa else 22 for a in seq])\n",
    "    seq = torch.cat([seq_tok, seq_empty], dim=0)#.unsqueeze(0)\n",
    "    if return_mask:\n",
    "        mask = torch.zeros_like(seq).bool()\n",
    "        mask[0:seq_len+1] = True\n",
    "        return seq, mask\n",
    "    else:\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fossil-indicator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:48.765283Z",
     "start_time": "2021-04-14T19:12:48.760182Z"
    }
   },
   "outputs": [],
   "source": [
    "aa_seq = \"ACDEFGHIKLMNOPQRSTUVWYZZZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "smooth-andrews",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:48.976532Z",
     "start_time": "2021-04-14T19:12:48.966152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "         14., 15., 16., 17., 18., 19., 20., 21., 22., 22., 22.,  0.,  0.,  0.,\n",
       "          0.,  0.]),\n",
       " tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True, False, False, False, False]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_aa_tokenizer(aa_seq, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-attachment",
   "metadata": {},
   "source": [
    "## Text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "obvious-honolulu",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:49.776848Z",
     "start_time": "2021-04-14T19:12:49.661970Z"
    }
   },
   "outputs": [],
   "source": [
    "from simple_tokenizer import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "functional-mexico",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:49.992504Z",
     "start_time": "2021-04-14T19:12:49.980382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5334, 11649, 22279,    73, 14134,   256,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(text, context_length=30, return_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "excessive-interface",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:50.241303Z",
     "start_time": "2021-04-14T19:12:50.235017Z"
    }
   },
   "outputs": [],
   "source": [
    "t, m = tokenize(text, context_length=30, return_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-chicago",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "separate-climb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:08:22.325290Z",
     "start_time": "2021-04-14T19:08:22.318073Z"
    }
   },
   "outputs": [],
   "source": [
    "class CLASPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Basic CLASP dataset that loads the preprocessed csv file into RAM.\n",
    "        path: path to the csv file\n",
    "    \"\"\"\n",
    "    def __init__(self, path, text_sampler, bioseq_sampler, text_tok, bioseq_tok):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.path = path\n",
    "\n",
    "        tp = time.time()\n",
    "        with open(path, \"r\") as reader:\n",
    "            self.data = reader.readlines()\n",
    "        print(f\"Load data time: {time.time() - tp:.3f} s\")\n",
    "\n",
    "        self.cols = self.data.pop(0).split(\",\")\n",
    "        self.len  = len(self.data)\n",
    "\n",
    "        self.text_sampler   = text_sampler\n",
    "        self.bioseq_sampler = bioseq_sampler\n",
    "\n",
    "        self.text_tok   = text_tok\n",
    "        self.bioseq_tok = bioseq_tok\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx][:-2] # without \"\\n\"\n",
    "        sample = sample.split(\",\")\n",
    "        sample = [x for x in sample if len(x) > 0]\n",
    "\n",
    "        text   = \" \".join(sample[:-2])\n",
    "        bioseq = sample[-1]\n",
    "\n",
    "        text   = self.text_sampler(text)\n",
    "        bioseq = self.bioseq_sampler(bioseq)\n",
    "        \n",
    "#        print(text)\n",
    "#        print(bioseq)\n",
    "\n",
    "        text, text_mask = self.text_tok(text)\n",
    "        bioseq, bioseq_mask = self.bioseq_tok(bioseq)\n",
    "\n",
    "        return text, text_mask, bioseq, bioseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "frozen-lebanon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:08:23.243549Z",
     "start_time": "2021-04-14T19:08:23.237124Z"
    }
   },
   "outputs": [],
   "source": [
    "str_sampler = partial(basic_rand_sampler, sample_len=100)\n",
    "text_tok    = partial(tokenize, context_length=120, return_mask=True)\n",
    "bioseq_tok  = partial(basic_aa_tokenizer, context_length=120, return_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "indoor-prescription",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:08:23.631801Z",
     "start_time": "2021-04-14T19:08:23.623519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data time: 0.002 s\n"
     ]
    }
   ],
   "source": [
    "ds = CLASPDataset(path=\"uniprot_100_reduced.csv\",\n",
    "                  text_sampler=str_sampler,\n",
    "                  bioseq_sampler=str_sampler,\n",
    "                  text_tok=text_tok,\n",
    "                  bioseq_tok=bioseq_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "stuffed-hacker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:08:24.250532Z",
     "start_time": "2021-04-14T19:08:24.238600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  331, 17650,  8729,  1095,  7549, 17015,  3683,   537, 14909,  6321,\n",
       "          18001,   539,  8368,  1160,   675,  9460,  8741,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False]]),\n",
       " tensor([10., 19.,  3.,  0.,  9.,  7., 16.,  2.,  3.,  0.,  4.,  7.,  8.,  5.,\n",
       "          1.,  0.,  0.,  7.,  5.,  0.,  5.,  9.,  0., 19.,  5.,  9.,  0.,  5.,\n",
       "          7.,  5.,  0.,  5.,  0.,  5.,  3., 16.,  5.,  7.,  5.,  0.,  0.,  0.,\n",
       "         19.,  5.,  0.,  7.,  0.,  3.,  2., 15.,  5.,  4.,  9.,  5.,  9.,  5.,\n",
       "          9.,  9.,  4., 17., 19.,  7., 13.,  3., 17.,  7., 19.,  7.,  4.,  5.,\n",
       "          9., 19.,  7., 16.,  4.,  7.,  9., 10.,  4.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-steps",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "informed-ethics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:03:51.148903Z",
     "start_time": "2021-04-14T19:03:51.143602Z"
    }
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unlikely-diabetes",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:03:51.468567Z",
     "start_time": "2021-04-14T19:03:51.446436Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "informational-ready",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:03:51.838771Z",
     "start_time": "2021-04-14T19:03:51.830007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([32, 1, 120]),\n",
       " torch.Size([32, 1, 120]),\n",
       " torch.Size([32, 120]),\n",
       " torch.Size([32, 120])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b.shape for b in batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-rapid",
   "metadata": {},
   "source": [
    "# CLASPRankSplitDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "necessary-bundle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:54.933246Z",
     "start_time": "2021-04-14T19:12:54.928000Z"
    }
   },
   "outputs": [],
   "source": [
    "path_offset_dict = '/home/mmp/hdd1/ProTexCLIP/uniprot_sprot_offset_dict.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "empirical-baker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:55.392996Z",
     "start_time": "2021-04-14T19:12:55.154213Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(path_offset_dict, \"r\", encoding='utf-8') as data_file:    \n",
    "    offset_dict = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "living-credit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:55.396459Z",
     "start_time": "2021-04-14T19:12:55.394121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564278"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(offset_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "foreign-childhood",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:55.601721Z",
     "start_time": "2021-04-14T19:12:55.596646Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/home/mmp/hdd1/ProTexCLIP/uniprot_sprot.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mineral-contributor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:55.869139Z",
     "start_time": "2021-04-14T19:12:55.861942Z"
    }
   },
   "outputs": [],
   "source": [
    "class RankSplitDataset(Dataset):\n",
    "    def __init__(self, file_path, offset_dict, rank, world_size):\n",
    "        self.file_path        = file_path\n",
    "        self.offset_dict      = offset_dict\n",
    "        self.total_len        = len(offset_dict.keys())\n",
    "        self.rank_len         = self.total_len // world_size\n",
    "        self.rank_line_offset = self.rank_len * rank\n",
    "        self.rank_byte_offset = self.offset_dict[str(self.rank_line_offset)] # because json keys are strings after it is saved\n",
    "        \n",
    "        print(f\"rank: {rank:<5}\")\n",
    "        print(f\"total len: {self.total_len}\")\n",
    "        print(f\"rank len: {self.rank_len}\")\n",
    "        print(f\"rank line offset: {self.rank_line_offset}\")\n",
    "        print(f\"rank byte offset: {self.rank_byte_offset}\")\n",
    "        \n",
    "        tp = time.time()\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            f.seek(self.rank_byte_offset) # move to the line for the specific rank\n",
    "            lines = []\n",
    "            for i in range(self.rank_len): # load all the lines for the rank\n",
    "                lines.append(f.readline())\n",
    "        print(f\"dataset load data time: {time.time() - tp:.3f} s\")\n",
    "        \n",
    "        self.data = lines\n",
    "        print(f\"dataset len: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "organic-black",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:56.325793Z",
     "start_time": "2021-04-14T19:12:56.319382Z"
    }
   },
   "outputs": [],
   "source": [
    "class CLASPRankSplitDataset(RankSplitDataset):\n",
    "    \"\"\"\n",
    "    CLASP rank split dataset that loads equally sized pieces for each rank\n",
    "    of the preprocessed csv file into RAM.\n",
    "        path: path to the csv file\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, offset_dict, rank, world_size,\n",
    "                 text_sampler, bioseq_sampler, text_tok, bioseq_tok):\n",
    "        super().__init__(file_path, offset_dict, rank, world_size)\n",
    "        \n",
    "        self.text_sampler   = text_sampler\n",
    "        self.bioseq_sampler = bioseq_sampler\n",
    "\n",
    "        self.text_tok   = text_tok\n",
    "        self.bioseq_tok = bioseq_tok\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx][:-2] # without \"\\n\"\n",
    "        sample = sample.split(\",\")\n",
    "        sample = [x for x in sample if len(x) > 0]\n",
    "\n",
    "        text   = \" \".join(sample[:-2])\n",
    "        bioseq = sample[-1]\n",
    "\n",
    "        text   = self.text_sampler(text)\n",
    "        bioseq = self.bioseq_sampler(bioseq)\n",
    "\n",
    "        text, text_mask = self.text_tok(text)\n",
    "        bioseq, bioseq_mask = self.bioseq_tok(bioseq)\n",
    "\n",
    "        return text, text_mask, bioseq, bioseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "previous-throw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:56.732043Z",
     "start_time": "2021-04-14T19:12:56.726823Z"
    }
   },
   "outputs": [],
   "source": [
    "str_sampler = partial(basic_rand_sampler, sample_len=100)\n",
    "text_tok    = partial(tokenize, context_length=120, return_mask=True)\n",
    "bioseq_tok  = partial(basic_aa_tokenizer, context_length=120, return_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "static-arrival",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:57.457111Z",
     "start_time": "2021-04-14T19:12:57.203144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           31Gi       9.4Gi        17Gi       0.0Ki       4.8Gi        21Gi\r\n",
      "Swap:         979Mi       973Mi       6.0Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "stunning-chest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:12:59.282931Z",
     "start_time": "2021-04-14T19:12:57.854234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total len: 564278\n",
      "rank len: 282139\n",
      "rank line offset: 0\n",
      "rank byte offset: 118\n",
      "dataset load data time: 1.423 s\n",
      "dataset len: 282139\n"
     ]
    }
   ],
   "source": [
    "ds1 = CLASPRankSplitDataset(file_path=file_path,\n",
    "                           offset_dict=offset_dict,\n",
    "                           rank=0,\n",
    "                           world_size=2,\n",
    "                           text_sampler=str_sampler,\n",
    "                           bioseq_sampler=str_sampler,\n",
    "                           text_tok=text_tok,\n",
    "                           bioseq_tok=bioseq_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sufficient-porcelain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:13:01.081161Z",
     "start_time": "2021-04-14T19:13:00.790214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           31Gi        10Gi        15Gi       0.0Ki       4.8Gi        19Gi\r\n",
      "Swap:         979Mi       973Mi       6.0Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "surface-chicken",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:13:00.789051Z",
     "start_time": "2021-04-14T19:12:59.561512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total len: 564278\n",
      "rank len: 282139\n",
      "rank line offset: 282139\n",
      "rank byte offset: 1520906478\n",
      "dataset load data time: 1.221 s\n",
      "dataset len: 282139\n"
     ]
    }
   ],
   "source": [
    "ds2 = CLASPRankSplitDataset(file_path=file_path,\n",
    "                           offset_dict=offset_dict,\n",
    "                           rank=1,\n",
    "                           world_size=2,\n",
    "                           text_sampler=str_sampler,\n",
    "                           bioseq_sampler=str_sampler,\n",
    "                           text_tok=text_tok,\n",
    "                           bioseq_tok=bioseq_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rising-powell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:13:01.081161Z",
     "start_time": "2021-04-14T19:13:00.790214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           31Gi        12Gi        14Gi       0.0Ki       4.8Gi        18Gi\r\n",
      "Swap:         979Mi       973Mi       6.0Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "scenic-association",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:13:01.177678Z",
     "start_time": "2021-04-14T19:13:01.087272Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-670d4b462cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mds2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "ds1[0] == ds2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "political-bookmark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:40:48.396297Z",
     "start_time": "2021-04-14T19:40:48.387955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  278,   271,   268,   279,   275,   263,   273,   271,   271,   275,\n",
       "           1818,    12, 43499,  8093,   281, 48765, 26769,   269,   346, 10077,\n",
       "            281,   271,   271,   271,   271,   274,   271,   276,    92,   269,\n",
       "           3692,   331,   282,   551,   276,   275,   279,   275,   279,   275,\n",
       "            282, 35004,   271,   280,   277,   277,   271,   269,   272,   282,\n",
       "             12,   282,   326,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False]]),\n",
       " tensor([16.,  9., 21., 21., 13., 11.,  2., 15.,  8.,  9.,  9.,  2., 21.,  8.,\n",
       "          3., 20., 16., 13., 13., 15., 19., 14., 19.,  3.,  1., 13.,  8.,  0.,\n",
       "         13., 19.,  3., 20., 11., 11., 13., 13., 16.,  3.,  8.,  5.,  9.,  7.,\n",
       "         19.,  5.,  6.,  4., 16.,  5.,  7.,  8., 21.,  8.,  5.,  3.,  8.,  0.,\n",
       "         14.,  0., 16.,  3., 19.,  2., 19., 11.,  8., 10.,  1.,  1., 20., 19.,\n",
       "         16.,  8.,  4.,  8.,  2.,  0., 10., 15., 15., 21., 14.,  5.,  7., 14.,\n",
       "         17.,  1.,  8.,  7., 13.,  5.,  8., 19.,  9., 16.,  2.,  9.,  2.,  0.,\n",
       "          8.,  7.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "together-labor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T19:40:48.867810Z",
     "start_time": "2021-04-14T19:40:48.860429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1735,   282,  1783,  4853,    78,  9429,  1160,   320,   281,   328,\n",
       "            269, 18833, 17640,   537,   841,  2343,   324,  8902, 18650,   282,\n",
       "          25693,  1029, 27751,   934,   269,   274,   277,   281,   273,   274,\n",
       "            277,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False]]),\n",
       " tensor([19.,  0., 13.,  3.,  1.,  4., 16.,  9.,  1., 16., 21.,  9.,  9., 16.,\n",
       "          5., 21., 17.,  8.,  8.,  2., 19., 15., 16., 11.,  3.,  0., 17., 10.,\n",
       "          8., 21.,  9.,  9., 10.,  5.,  5.,  0., 16., 16., 16.,  7.,  9., 19.,\n",
       "          6.,  5.,  4., 16., 20.,  9., 21.,  5., 16., 16.,  5.,  5.,  3.,  7.,\n",
       "          3.,  9., 14.,  3.,  7., 19., 11.,  5.,  9.,  7., 11., 17., 14., 10.,\n",
       "         21., 11., 16., 13.,  5.,  7., 16.,  7.,  0.,  9.,  7.,  4.,  7., 17.,\n",
       "         19.,  5.,  7.,  5.,  4.,  8.,  9., 16., 13.,  0., 13., 16.,  6., 14.,\n",
       "         20., 17.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-polyester",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.8.0",
   "language": "python",
   "name": "pytorch1.8.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
